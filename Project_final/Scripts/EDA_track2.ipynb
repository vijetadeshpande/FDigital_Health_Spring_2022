{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acc4c15f-d4c4-44dc-92dd-605cca7e6bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d4cdf7c-f2b3-4889-b775-1c62e952a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "path_parent = r'/Users/vijetadeshpande/Downloads/UMass Lowell - Courses/Spring 2022/Foundations in Digital Health/FDigital_Health_Spring_2022/Project_final'\n",
    "path_data = os.path.join(path_parent, r'Data/Track2_SubtaskA/Annotations')\n",
    "path_train = os.path.join(path_data, r'train/mimic')\n",
    "path_val = os.path.join(path_data, r'dev/mimic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "134600e3-2d04-4ef8-b8a6-ed55ab064d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2632\n",
      "376\n"
     ]
    }
   ],
   "source": [
    "# first read all files\n",
    "files_train = []\n",
    "files_val = []\n",
    "\n",
    "files_train += glob.glob(path_train + \"/*.txt\")\n",
    "files_train += glob.glob(path_train + \"/*.ann\")\n",
    "\n",
    "files_val += glob.glob(path_val + \"/*.txt\")\n",
    "files_val += glob.glob(path_val + \"/*.ann\")\n",
    "\n",
    "#\n",
    "print(len(files_train))\n",
    "print(len(files_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baee2dd4-c847-450e-82c9-7d40d19e0cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/vijetadeshpande/.cache/huggingface/modules/datasets_modules/datasets/sem_eval_2010_task_8/8545d1995bbbade386acf5c4e2bef5589d8387ae0a93356407dfb54cdb234416 (last modified on Sat Apr 23 12:30:09 2022) since it couldn't be found locally at sem_eval_2010_task_8., or remotely on the Hugging Face Hub.\n",
      "Using custom data configuration default\n",
      "Reusing dataset sem_eval2010_task8 (/Users/vijetadeshpande/.cache/huggingface/datasets/sem_eval2010_task8/default/1.0.0/8545d1995bbbade386acf5c4e2bef5589d8387ae0a93356407dfb54cdb234416)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3470b27c4176467ebf12d6544a312c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ConnectionError",
     "evalue": "Couldn't reach 'xiaobendanyn/tacred' on the Hub (ConnectionError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8l/1lvr0w3s7j5b1k19qx5kmm6m0000gn/T/ipykernel_48448/431456160.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msem_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sem_eval_2010_task_8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdoc_red\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xiaobendanyn/tacred'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, script_version, **config_kwargs)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1675\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1676\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, script_version, **config_kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_auth_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1512\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1513\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m     )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1201\u001b[0m                         \u001b[0;34mf\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                     ) from None\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m         raise FileNotFoundError(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1168\u001b[0m                         ),\n\u001b[1;32m   1169\u001b[0m                     ):\n\u001b[0;32m-> 1170\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't reach '{path}' on the Hub ({type(e).__name__})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1171\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0;34m\"404\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Dataset '{path}' doesn't exist on the Hub\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: Couldn't reach 'xiaobendanyn/tacred' on the Hub (ConnectionError)"
     ]
    }
   ],
   "source": [
    "# first load similar dataset to see the characteristics of the NER and RE data\n",
    "\n",
    "sem_eval = datasets.load_dataset('sem_eval_2010_task_8')\n",
    "doc_red = datasets.load_dataset('xiaobendanyn/tacred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88913e08-c01b-4869-b2a9-b2952a4fecb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_red' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8l/1lvr0w3s7j5b1k19qx5kmm6m0000gn/T/ipykernel_48448/1182945167.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(sem_eval['train'][200])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_red\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'doc_red' is not defined"
     ]
    }
   ],
   "source": [
    "#\n",
    "#print(sem_eval['train'][200])\n",
    "print(doc_red['train'][200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35743e0f-ddbd-436d-95b0-fb05c4206859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====\n",
      "Social History:\n",
      "\n",
      "=====\n",
      "Social History:\n",
      "The patient has smoked for 27 years, an unspecified amount. She\n",
      "does not currently drink but has a long history of ethanol\n",
      "abuse.  She is disabled.\n",
      "T1\tTobacco 32 38\tsmoked\n",
      "T2\tDuration 39 51\tfor 27 years\n",
      "T3\tStatusTime 28 38\thas smoked\n",
      "T4\tAlcohol 131 138;139 144\tethanol abuse\n",
      "T5\tStatusTime 80 127\tdoes not currently drink but has a long history\n",
      "E1\tTobacco:T1 Status:T3 Duration:T2\n",
      "E2\tAlcohol:T4 Status:T5\n",
      "A1\tStatusTimeVal T3 current\n",
      "A2\tStatusTimeVal T5 past\n",
      "T6\tEmployment 154 162\tdisabled\n",
      "E3\tEmployment:T6 Status:T7\n",
      "T7\tStatusEmploy 154 162\tdisabled\n",
      "A3\tStatusEmployVal T7 on_disability\n",
      "\n",
      "=====\n",
      "Social History:\n",
      "Lives with:alone, Can stay at nephews or brothers if needed\n",
      "Contact:[**Name (NI) **] (Brother) Phone #\n",
      "Occupation:inspector at R&D Engineering\n",
      "Cigarettes: Smoked no [] yes [x] last cigarette [**2100-8-11**] Hx:<1ppd\n",
      "x 40 years, Has quit in past for up to 3 yrs\n",
      "Other Tobacco use:denies\n",
      "ETOH: Recovered alcoholic. No ETOH in 22 years\n",
      "Illicit drug use:denies\n",
      "T1\tLivingStatus 16 21\tLives\n",
      "T2\tStatusTime 16 21\tLives\n",
      "T3\tTypeLiving 27 32\talone\n",
      "T4\tEmployment 119 129\tOccupation\n",
      "T5\tType 130 158\tinspector at R&D Engineering\n",
      "T6\tStatusEmploy 130 158\tinspector at R&D Engineering\n",
      "T7\tTobacco 159 169\tCigarettes\n",
      "T8\tType 159 169\tCigarettes\n",
      "T9\tStatusTime 192 206\tlast cigarette\n",
      "T10\tHistory 207 222\t[**2100-8-11**]\n",
      "T11\tAmount 223 231\tHx:<1ppd\n",
      "T12\tDuration 232 242\tx 40 years\n",
      "T13\tTobacco 277 294\tOther Tobacco use\n",
      "T14\tType 277 282\tOther\n",
      "T15\tStatusTime 295 301\tdenies\n",
      "T16\tAlcohol 302 306\tETOH\n",
      "T18\tStatusTime 308 327\tRecovered alcoholic\n",
      "T19\tAlcohol 332 336\tETOH\n",
      "T20\tStatusTime 329 348\tNo ETOH in 22 years\n",
      "T21\tHistory 337 348\tin 22 years\n",
      "T22\tDrug 349 365\tIllicit drug use\n",
      "T23\tType 349 356\tIllicit\n",
      "T24\tStatusTime 366 372\tdenies\n",
      "E1\tLivingStatus:T1 Status:T2 Type:T3\n",
      "E2\tEmployment:T4 Status:T6 Type:T5\n",
      "E3\tTobacco:T7 Type:T8 Status:T9 History:T10 Amount:T11 Duration:T12\n",
      "E4\tTobacco:T13 Type:T14 Status:T15\n",
      "E5\tAlcohol:T16 Status:T18\n",
      "E6\tAlcohol:T19 Status:T20 History:T21\n",
      "E7\tDrug:T22 Type:T23 Status:T24\n",
      "A1\tStatusTimeVal T2 current\n",
      "A2\tTypeLivingVal T3 alone\n",
      "A3\tStatusEmployVal T6 employed\n",
      "A4\tStatusTimeVal T9 past\n",
      "A5\tStatusTimeVal T15 none\n",
      "A7\tStatusTimeVal T18 past\n",
      "A8\tStatusTimeVal T20 past\n",
      "A9\tStatusTimeVal T24 none\n",
      "\n",
      "=====\n",
      "Social History:\n",
      "h/o smoking, EtoH, marijuana and cocaine.  Denies currently.\n",
      "Lives at [**Location 4367**] [**Hospital3 **].\n",
      "T1\tTobacco 20 27\tsmoking\n",
      "T2\tAlcohol 29 33\tEtoH\n",
      "T3\tDrug 35 56\tmarijuana and cocaine\n",
      "T4\tType 35 56\tmarijuana and cocaine\n",
      "T5\tStatusTime 16 19\th/o\n",
      "E1\tTobacco:T1 Status:T5\n",
      "E2\tAlcohol:T2 Status:T5\n",
      "E3\tDrug:T3 Type:T4 Status:T5\n",
      "A1\tStatusTimeVal T5 current\n",
      "\n",
      "=====\n",
      "Social History:\n",
      "Alcohol: 2 drinks/night.\n",
      "Tobacco: 50 pack-years. Currently still smoking.\n",
      "Drugs: Denies.\n",
      "Currently retired. Lives alone without assistance. Daughters in\n",
      "the area. Used to work as a secretary at a lumber mill.\n",
      "T1\tAlcohol 16 23\tAlcohol\n",
      "T2\tStatusTime 25 39\t2 drinks/night\n",
      "T3\tAmount 25 33\t2 drinks\n",
      "T4\tFrequency 33 39\t/night\n",
      "T5\tTobacco 41 48\tTobacco\n",
      "T6\tAmount 50 63\t50 pack-years\n",
      "T7\tStatusTime 50 63\t50 pack-years\n",
      "T8\tTobacco 65 88\tCurrently still smoking\n",
      "T9\tStatusTime 65 88\tCurrently still smoking\n",
      "T10\tDrug 90 95\tDrugs\n",
      "T11\tStatusTime 97 103\tDenies\n",
      "T12\tEmployment 105 122\tCurrently retired\n",
      "T13\tStatusEmploy 105 122\tCurrently retired\n",
      "T14\tLivingStatus 124 129\tLives\n",
      "T15\tStatusTime 124 129\tLives\n",
      "T16\tTypeLiving 130 135\talone\n",
      "T17\tEmployment 187 191\twork\n",
      "T18\tStatusEmploy 179 191\tUsed to work\n",
      "T19\tType 195 206\ta secretary\n",
      "E1\tAlcohol:T1 Amount:T3 Status:T2 Frequency:T4\n",
      "E2\tTobacco:T5 Amount:T6 Status:T7\n",
      "E3\tTobacco:T8 Status:T9\n",
      "E4\tDrug:T10 Status:T11\n",
      "E5\tEmployment:T12 Status:T13\n",
      "E6\tLivingStatus:T14 Status:T15 Type:T16\n",
      "E7\tEmployment:T17 Status:T18 Type:T19\n",
      "A1\tStatusTimeVal T2 current\n",
      "A2\tStatusTimeVal T7 current\n",
      "A3\tStatusTimeVal T9 current\n",
      "A4\tStatusTimeVal T11 none\n",
      "A5\tStatusEmployVal T13 retired\n",
      "A6\tStatusTimeVal T15 current\n",
      "A7\tTypeLivingVal T16 alone\n",
      "A8\tStatusEmployVal T18 unemployed\n",
      "\n",
      "=====\n",
      "Social History:\n",
      "Lives at home with wife.\n",
      "Plays clarinet and tenor saxophone.\n",
      "Owns office coffee business.\n",
      "Smoked 1 ppd for between 30 and 40 years but stopped 26 years\n",
      "ago.\n",
      "Occ ETOH.\n",
      "No drugs\n",
      "T1\tLivingStatus 16 21\tLives\n",
      "T2\tStatusTime 16 21\tLives\n",
      "T3\tTypeLiving 30 39\twith wife\n",
      "T4\tTobacco 106 112\tSmoked\n",
      "T5\tAmount 113 118\t1 ppd\n",
      "T6\tDuration 119 146\tfor between 30 and 40 years\n",
      "T7\tStatusTime 151 158\tstopped\n",
      "T8\tHistory 159 167;168 171\t26 years ago\n",
      "T9\tAlcohol 177 181\tETOH\n",
      "T11\tStatusTime 173 176\tOcc\n",
      "T12\tDrug 186 191\tdrugs\n",
      "T13\tStatusTime 183 185\tNo\n",
      "T14\tEmployment 77 81\tOwns\n",
      "T15\tStatusEmploy 77 81\tOwns\n",
      "T16\tType 82 104\toffice coffee business\n",
      "E1\tLivingStatus:T1 Status:T2 Type:T3\n",
      "E2\tTobacco:T4 Amount:T5 Duration:T6 Status:T7 History:T8\n",
      "E3\tAlcohol:T9 Status:T11\n",
      "E4\tDrug:T12 Status:T13\n",
      "E5\tEmployment:T14 Status:T15 Type:T16\n",
      "A1\tStatusTimeVal T2 current\n",
      "A2\tTypeLivingVal T3 with_family\n",
      "A3\tStatusTimeVal T7 past\n",
      "A5\tStatusTimeVal T11 current\n",
      "A6\tStatusTimeVal T13 none\n",
      "A7\tStatusEmployVal T15 employed\n",
      "\n",
      "=====\n",
      "Social History:\n",
      "hx heavy drinking denies IVDU <1yr hx smoking lives with mom\n",
      "never married used to work as operation microscope repair man\n",
      "T1\tAlcohol 25 33\tdrinking\n",
      "T3\tStatusTime 16 18\thx\n",
      "T4\tDrug 41 45\tIVDU\n",
      "T5\tMethod 41 45\tIVDU\n",
      "T6\tStatusTime 34 40\tdenies\n",
      "T7\tTobacco 54 61\tsmoking\n",
      "T8\tDuration 46 50\t<1yr\n",
      "T9\tStatusTime 51 53\thx\n",
      "T10\tLivingStatus 62 67\tlives\n",
      "T11\tTypeLiving 68 76\twith mom\n",
      "T12\tStatusTime 62 67\tlives\n",
      "T13\tEmployment 99 103\twork\n",
      "T14\tStatusEmploy 91 98\tused to\n",
      "T15\tType 107 138\toperation microscope repair man\n",
      "E1\tAlcohol:T1 Status:T3\n",
      "E2\tDrug:T4 Status:T6 Method:T5\n",
      "E3\tTobacco:T7 Status:T9 Duration:T8\n",
      "E4\tLivingStatus:T10 Type:T11 Status:T12\n",
      "E5\tEmployment:T13 Status:T14 Type:T15\n",
      "A2\tStatusTimeVal T3 current\n",
      "A3\tStatusTimeVal T6 none\n",
      "A4\tStatusTimeVal T9 current\n",
      "A5\tTypeLivingVal T11 with_family\n",
      "A6\tStatusTimeVal T12 current\n",
      "A7\tStatusEmployVal T14 unemployed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the data in files\n",
    "data_train = {'x': [], 'y': []}\n",
    "\n",
    "for idx, file in enumerate(files_train):\n",
    "    if file[-3:] == 'ann':\n",
    "        break\n",
    "    \n",
    "    #\n",
    "    with open(file, 'r') as f:\n",
    "        x_ = f.read()\n",
    "    with open(file[:-3]+'ann', 'r') as f:\n",
    "        y_ = f.read()\n",
    "    \n",
    "    print(\"=\"*5)\n",
    "    print(x_)\n",
    "    print(y_)\n",
    "    \n",
    "    if idx > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7f55bb2-106d-475e-9600-6648764d4e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from above exmaples it is clear that there are multiple overlapping entities\n",
    "# therefore the task is not flat-NER, it is nested NER.\n",
    "# if we formulate the problem as MRC then we will need to come-up with questions for each kind of entity and respective attributes\n",
    "\n",
    "# sample query\n",
    "query_template = {\n",
    "    'status': '[ENT-TYPE] status?',\n",
    "    'duration': '[ENT-TYPE] duration?',\n",
    "    'history': '[ENT-TYPE] history?',\n",
    "    'type': '[ENT-TYPE]?',\n",
    "    'amount': '[ENT-TYPE] amount?',\n",
    "    'frequency': '[ENT-TYPE] frequency?',\n",
    "    'method': '[ENT-TYPE] method?',\n",
    "}\n",
    "\n",
    "# example of question\n",
    "query_dict = {\n",
    "    'tobacco': {},\n",
    "    'alcohol': {},\n",
    "    'drug': {},\n",
    "    'employment': {},\n",
    "    'living status': {},\n",
    "    'insurance': {},    \n",
    "    'sexual orientation': {},    \n",
    "    'gender identity': {},\n",
    "    'country of origin': {},\n",
    "    'race': {},\n",
    "    'physical activity': {},\n",
    "    'environmental exposure': {},\n",
    "}\n",
    "\n",
    "for ent in query_dict:\n",
    "    for attribute in query_template:\n",
    "        query_dict[ent][attribute] = query_template[attribute].replace('[ENT-TYPE]', ent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdf25b9b-b00c-432b-a308-96e40b0b11dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tobacco': {'status': 'tobacco status?', 'duration': 'tobacco duration?', 'history': 'tobacco history?', 'type': 'tobacco?', 'amount': 'tobacco amount?', 'frequency': 'tobacco frequency?', 'method': 'tobacco method?'}, 'alcohol': {'status': 'alcohol status?', 'duration': 'alcohol duration?', 'history': 'alcohol history?', 'type': 'alcohol?', 'amount': 'alcohol amount?', 'frequency': 'alcohol frequency?', 'method': 'alcohol method?'}, 'drug': {'status': 'drug status?', 'duration': 'drug duration?', 'history': 'drug history?', 'type': 'drug?', 'amount': 'drug amount?', 'frequency': 'drug frequency?', 'method': 'drug method?'}, 'employment': {'status': 'employment status?', 'duration': 'employment duration?', 'history': 'employment history?', 'type': 'employment?', 'amount': 'employment amount?', 'frequency': 'employment frequency?', 'method': 'employment method?'}, 'living status': {'status': 'living status status?', 'duration': 'living status duration?', 'history': 'living status history?', 'type': 'living status?', 'amount': 'living status amount?', 'frequency': 'living status frequency?', 'method': 'living status method?'}, 'insurance': {'status': 'insurance status?', 'duration': 'insurance duration?', 'history': 'insurance history?', 'type': 'insurance?', 'amount': 'insurance amount?', 'frequency': 'insurance frequency?', 'method': 'insurance method?'}, 'sexual orientation': {'status': 'sexual orientation status?', 'duration': 'sexual orientation duration?', 'history': 'sexual orientation history?', 'type': 'sexual orientation?', 'amount': 'sexual orientation amount?', 'frequency': 'sexual orientation frequency?', 'method': 'sexual orientation method?'}, 'gender identity': {'status': 'gender identity status?', 'duration': 'gender identity duration?', 'history': 'gender identity history?', 'type': 'gender identity?', 'amount': 'gender identity amount?', 'frequency': 'gender identity frequency?', 'method': 'gender identity method?'}, 'country of origin': {'status': 'country of origin status?', 'duration': 'country of origin duration?', 'history': 'country of origin history?', 'type': 'country of origin?', 'amount': 'country of origin amount?', 'frequency': 'country of origin frequency?', 'method': 'country of origin method?'}, 'race': {'status': 'race status?', 'duration': 'race duration?', 'history': 'race history?', 'type': 'race?', 'amount': 'race amount?', 'frequency': 'race frequency?', 'method': 'race method?'}, 'physical activity': {'status': 'physical activity status?', 'duration': 'physical activity duration?', 'history': 'physical activity history?', 'type': 'physical activity?', 'amount': 'physical activity amount?', 'frequency': 'physical activity frequency?', 'method': 'physical activity method?'}, 'environmental exposure': {'status': 'environmental exposure status?', 'duration': 'environmental exposure duration?', 'history': 'environmental exposure history?', 'type': 'environmental exposure?', 'amount': 'environmental exposure amount?', 'frequency': 'environmental exposure frequency?', 'method': 'environmental exposure method?'}}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "print(query_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58c0fbce-8153-44da-9302-82c86d3bdc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux functions\n",
    "def count_words(list_):\n",
    "    \n",
    "    words = 1\n",
    "    for element in list_:\n",
    "        if ';' in element and (not element.replace(';', '').isalpha()):\n",
    "            words += 1\n",
    "    \n",
    "    return words\n",
    "\n",
    "def deal_with_multi_words(list_, num_words):\n",
    "    list_out = []\n",
    "    for element in list_:\n",
    "        if ';' in element and (not element.replace(';', '').isalpha()):\n",
    "            eles = element.split(';')\n",
    "            \n",
    "            # check if sub-spans are next to each other or not\n",
    "            if (int(eles[0]) - int(eles[1])) > 1:\n",
    "                print('sub-spans are not next to each other')\n",
    "                exit()\n",
    "            \n",
    "            list_out += eles\n",
    "        else:\n",
    "            list_out += [element]\n",
    "    \n",
    "    return list_out\n",
    "\n",
    "def structurize_tags(list_):\n",
    "    \n",
    "    total_words = count_words(list_)\n",
    "    if total_words >= 2:\n",
    "        list_ = deal_with_multi_words(list_, total_words)\n",
    "    \n",
    "    #\n",
    "    span_start = list_[2]\n",
    "    span_end = list_[1 + (total_words*2)]\n",
    "    sub_spans = list_[2:1 + (total_words*2) + 1]\n",
    "    \n",
    "    #\n",
    "    dict_ = {\n",
    "        'tag index': list_[0],\n",
    "        'tag type': list_[1],\n",
    "        'span start': span_start,\n",
    "        'span end': span_end,\n",
    "        'sub spans': sub_spans,\n",
    "        'num words': total_words,\n",
    "        'text': ' '.join(list_[1 + (total_words*2) + 1: ]),\n",
    "    }\n",
    "    \n",
    "    return dict_\n",
    "\n",
    "def structurize_relations(list_):\n",
    "    \n",
    "    for n_, node in enumerate(list_[1:]):\n",
    "        if n_ == 0:\n",
    "            #this is the fixed start node for the current line in the annotation\n",
    "            continue\n",
    "        else:\n",
    "            dict_ = {\n",
    "                'relation index': list_[0],\n",
    "                'start node': list_[1].split(':')[-1],\n",
    "                'start node type': list_[1].split(':')[0],\n",
    "                'end node': list_[n_].split(':')[-1],\n",
    "                'end node type': list_[n_].split(':')[0],\n",
    "            }    \n",
    "    \n",
    "    return dict_\n",
    "\n",
    "def structurize_attributes(list_):\n",
    "    \n",
    "    dict_ = {\n",
    "        'attribute index': list_[0],\n",
    "        'start node': list_[2],\n",
    "        'start node type': list_[1],#.replace('Val', ''),\n",
    "        'text': list_[-1],\n",
    "    }    \n",
    "    \n",
    "    return dict_\n",
    "\n",
    "def get_positive_query():\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "612b26a7-3385-49ae-9200-2973c4f1a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation_dict(ann):\n",
    "    #print(ann)\n",
    "    annot_dict = {\n",
    "        'tags': [],\n",
    "        'edges': [],\n",
    "        'attributes': [],\n",
    "    }\n",
    "    \n",
    "    #\n",
    "    if ann == '':\n",
    "        return annot_dict\n",
    "    \n",
    "    ann = ann.split('\\n')\n",
    "    for line in ann:\n",
    "        if line == '':\n",
    "            continue\n",
    "        \n",
    "        #\n",
    "        line = line.replace(' ', '\\t')\n",
    "        annot = line.split('\\t')\n",
    "        #print(annot)\n",
    "        \n",
    "        #\n",
    "        if annot[0][0] == 'T':\n",
    "            annot_dict['tags'].append(structurize_tags(annot))\n",
    "        elif annot[0][0] == 'E':\n",
    "            annot_dict['edges'].append(structurize_relations(annot))\n",
    "        elif annot[0][0] == 'A':\n",
    "            annot_dict['attributes'].append(structurize_attributes(annot))\n",
    "        else:\n",
    "            print('Feature other than tag, relation and attribute is present')\n",
    "            \n",
    "    return annot_dict\n",
    "\n",
    "def get_qca_triplets(context, label_dict, query_dict):\n",
    "    instances = []\n",
    "    \n",
    "    # first let's deal with tag type\n",
    "    for ent in label_dict['tags']:\n",
    "        query_pos = get_positive_query(ent, query_dict)\n",
    "        instances.append(\n",
    "            {\n",
    "                'context': context,\n",
    "                'query': query,\n",
    "                'answer start': ent['span start'],\n",
    "                'answer end': ent['span end'],\n",
    "                'answer text': ent['text'],\n",
    "                'answer entity type': ent['tag type']\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e26c5d9-016f-4bba-a203-822d66d39f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for every instance in the dataset, we need (query, context, answer) triplets\n",
    "\n",
    "# read the data in files\n",
    "data_train = []\n",
    "structured_annotations = []\n",
    "empty_annot = 0\n",
    "\n",
    "for subset in [files_train, files_val]:\n",
    "    for idx, file in enumerate(subset):\n",
    "        if file[-3:] == 'ann':\n",
    "            continue\n",
    "\n",
    "        # key for data instance\n",
    "        key = file[:-4]\n",
    "\n",
    "        # read text and annotations\n",
    "        with open(file, 'r') as f:\n",
    "            x_ = f.read()\n",
    "        with open(file[:-3]+'ann', 'r') as f:\n",
    "            y_ = f.read()\n",
    "\n",
    "        # if annotations are empty then continue to next file\n",
    "        if y_ == '':\n",
    "            empty_annot += 1\n",
    "            continue\n",
    "\n",
    "        # structurize the annotations\n",
    "        y_dict = get_annotation_dict(y_)\n",
    "        y_dict['instance key'] = key\n",
    "        structured_annotations.append(y_dict)\n",
    "\n",
    "        # y_dict has three keys, 'tags', 'edges' and 'attributes'. Each key has list form form of value\n",
    "        # we will have one query for each  element in 'tags' and 'edges'\n",
    "        # I DON'T KNOW WHAT TO DO WITH 'attributes'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e07e780-9b92-4e4d-a1d1-3713fd0724de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1451\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "print(len(structured_annotations))\n",
    "print(empty_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d6a945d0-ab83-4901-a8b2-f62da9b7d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect unique tag types and unique attribute types\n",
    "unique_tags = {}\n",
    "unique_att = {}\n",
    "\n",
    "for example in structured_annotations:\n",
    "    for tag in example['tags']:\n",
    "        tt_ = tag['tag type']\n",
    "        if tt_ in unique_tags:\n",
    "            unique_tags[tt_] += 1\n",
    "        else:\n",
    "            unique_tags[tt_] = 1\n",
    "    \n",
    "    for att in example['attributes']:\n",
    "        node = att['start node type']\n",
    "        if node in unique_att:\n",
    "            unique_att[node] += 1\n",
    "        else:\n",
    "            unique_att[node] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0d6465f-b56b-4bdf-9506-46dbf834d825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'StatusTimeVal': 4617, 'StatusEmployVal': 1072, 'TypeLivingVal': 1076}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9ce96e1-7940-407a-a53d-c6407ccb61c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tobacco': 1381,\n",
       " 'Duration': 531,\n",
       " 'StatusTime': 4617,\n",
       " 'Alcohol': 1455,\n",
       " 'Employment': 1072,\n",
       " 'StatusEmploy': 1072,\n",
       " 'LivingStatus': 1075,\n",
       " 'TypeLiving': 1076,\n",
       " 'Type': 1904,\n",
       " 'History': 687,\n",
       " 'Amount': 1007,\n",
       " 'Drug': 1087,\n",
       " 'Frequency': 602,\n",
       " 'Method': 217}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e99c3-52a4-48a5-a48d-c90ebd9bbb25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
